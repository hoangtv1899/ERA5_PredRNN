2022-07-20 16:19:20.607519: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home1/08589/hvtran/.local/lib/python3.9/site-packages/cv2/../../lib64:/opt/apps/pmix/3.2.3/lib:/opt/apps/intel19/python3/3.9.7/lib:/opt/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/libfabric/lib:/opt/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/lib/release:/opt/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/lib:/opt/intel/debugger_2020/libipt/intel64/lib:/opt/intel/compilers_and_libraries_2020.1.217/linux/daal/lib/intel64_lin:/opt/intel/compilers_and_libraries_2020.1.217/linux/tbb/lib/intel64_lin/gcc4.8:/opt/intel/compilers_and_libraries_2020.1.217/linux/mkl/lib/intel64_lin:/opt/intel/compilers_and_libraries_2020.1.217/linux/ipp/lib/intel64:/opt/intel/compilers_and_libraries_2020.1.217/linux/compiler/lib/intel64_lin:/opt/apps/gcc/9.4.0/lib64:/opt/apps/gcc/9.4.0/lib:/usr/lib64
2022-07-20 16:19:20.608101: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /home1/08589/hvtran/.local/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth
Namespace(is_training=1, device='cuda', dataset_name='mnist', train_data_paths='/work/08589/hvtran/ls6/ERA5_PredRNN-main/era5_train_2015_6_12hr.npz', valid_data_paths='/work/08589/hvtran/ls6/ERA5_PredRNN-main/era5_train_5_12hr.npz', save_dir='/work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn', gen_frm_dir='/work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn', input_length=12, total_length=24, img_width=1440, img_height=720, img_channel=6, concurent_step=1, use_weight=1, layer_weight='10,10,10,10,20,20', model_name='predrnn_v2', pretrained_model='/work/08589/hvtran/ls6/ERA5_PredRNN-main/model.ckpt-400', num_hidden='256,256,256,256', filter_size=5, stride=1, patch_size=20, patch_size1=4, layer_norm=1, decouple_beta=0.05, reverse_scheduled_sampling=1, r_sampling_step_1=25000.0, r_sampling_step_2=50000, r_exp_alpha=2500, scheduled_sampling=1, sampling_stop_iter=50000, sampling_start_value=1.0, sampling_changing_rate=2e-05, lr=0.00025, reverse_input=1, batch_size=1, max_iterations=5000, display_interval=10, test_interval=1000000, snapshot_interval=200, num_save_samples=10, n_gpu=1, visual=0, visual_path='./decoupling_visual', injection_action='concat', conv_on_input=0, res_on_conv=0, num_action_ch=4, is_static=0, is_scale=0, out_scale1='', out_scale2='', in_scale1='', in_scale2='', noise_val=0, out_channel=5, stat_layers=8, stat_layers2=5, out_weights='', curr_best_loss=1000000.0, isloss=1, is_logscale=0, is_WV=1)
Initializing models
load model: /work/08589/hvtran/ls6/ERA5_PredRNN-main/model.ckpt-400
input_raw_data
(720, 5, 720, 1440)
dims
(1, 3)
clips
(2, 60, 2)
input_raw_data
(720, 6, 720, 1440)
dims
(1, 3)
clips
(2, 60, 2)
2022-07-20 16:24:19 itr: 10
training loss: 11.329854965209961
2022-07-20 16:26:29 itr: 20
training loss: 11.418460845947266
2022-07-20 16:28:39 itr: 30
training loss: 11.33588981628418
2022-07-20 16:30:49 itr: 40
training loss: 11.302756309509277
2022-07-20 16:32:58 itr: 50
training loss: 11.164146423339844
2022-07-20 16:35:08 itr: 60
training loss: 11.213224411010742
2022-07-20 16:37:18 itr: 70
training loss: 11.16241455078125
2022-07-20 16:39:27 itr: 80
training loss: 11.14564323425293
2022-07-20 16:41:37 itr: 90
training loss: 11.101973533630371
2022-07-20 16:43:46 itr: 100
training loss: 11.085850715637207
2022-07-20 16:45:55 itr: 110
training loss: 11.089839935302734
2022-07-20 16:48:04 itr: 120
training loss: 11.103879928588867
2022-07-20 16:50:13 itr: 130
training loss: 11.030902862548828
2022-07-20 16:52:23 itr: 140
training loss: 11.060312271118164
2022-07-20 16:54:32 itr: 150
training loss: 11.119163513183594
2022-07-20 16:56:41 itr: 160
training loss: 11.047660827636719
2022-07-20 16:58:50 itr: 170
training loss: 10.998873710632324
2022-07-20 17:00:59 itr: 180
training loss: 11.11163330078125
2022-07-20 17:03:08 itr: 190
training loss: 11.088069915771484
2022-07-20 17:05:17 itr: 200
training loss: 11.049690246582031
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-200
2022-07-20 17:07:29 itr: 210
training loss: 11.11272144317627
2022-07-20 17:09:38 itr: 220
training loss: 11.092692375183105
2022-07-20 17:11:48 itr: 230
training loss: 11.071357727050781
2022-07-20 17:13:57 itr: 240
training loss: 11.006065368652344
2022-07-20 17:16:07 itr: 250
training loss: 11.028843879699707
2022-07-20 17:18:17 itr: 260
training loss: 11.00477409362793
2022-07-20 17:20:26 itr: 270
training loss: 11.056702613830566
2022-07-20 17:22:36 itr: 280
training loss: 11.067098617553711
2022-07-20 17:24:46 itr: 290
training loss: 11.073358535766602
2022-07-20 17:26:56 itr: 300
training loss: 11.06661605834961
2022-07-20 17:29:05 itr: 310
training loss: 11.09979248046875
2022-07-20 17:31:15 itr: 320
training loss: 11.115385055541992
2022-07-20 17:33:25 itr: 330
training loss: 11.13803482055664
2022-07-20 17:35:35 itr: 340
training loss: 11.088237762451172
2022-07-20 17:37:45 itr: 350
training loss: 11.079095840454102
2022-07-20 17:39:54 itr: 360
training loss: 11.101543426513672
2022-07-20 17:42:04 itr: 370
training loss: 11.087078094482422
2022-07-20 17:44:14 itr: 380
training loss: 11.066076278686523
2022-07-20 17:46:23 itr: 390
training loss: 11.104608535766602
2022-07-20 17:48:33 itr: 400
training loss: 11.095455169677734
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-400
2022-07-20 17:50:45 itr: 410
training loss: 11.080358505249023
2022-07-20 17:52:55 itr: 420
training loss: 11.087099075317383
2022-07-20 17:55:05 itr: 430
training loss: 11.06334114074707
2022-07-20 17:57:15 itr: 440
training loss: 11.105685234069824
2022-07-20 17:59:25 itr: 450
training loss: 11.074078559875488
2022-07-20 18:01:36 itr: 460
training loss: 11.098968505859375
2022-07-20 18:03:46 itr: 470
training loss: 11.098363876342773
2022-07-20 18:05:57 itr: 480
training loss: 11.13035774230957
2022-07-20 18:08:08 itr: 490
training loss: 11.059622764587402
2022-07-20 18:10:18 itr: 500
training loss: 11.120233535766602
2022-07-20 18:12:28 itr: 510
training loss: 11.092568397521973
2022-07-20 18:14:39 itr: 520
training loss: 11.120274543762207
2022-07-20 18:16:50 itr: 530
training loss: 11.112940788269043
2022-07-20 18:19:00 itr: 540
training loss: 11.167430877685547
2022-07-20 18:21:11 itr: 550
training loss: 11.148723602294922
2022-07-20 18:23:21 itr: 560
training loss: 11.122608184814453
2022-07-20 18:25:30 itr: 570
training loss: 11.102086067199707
2022-07-20 18:27:39 itr: 580
training loss: 11.076398849487305
2022-07-20 18:29:49 itr: 590
training loss: 11.103374481201172
2022-07-20 18:32:00 itr: 600
training loss: 11.152503967285156
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-600
2022-07-20 18:34:13 itr: 610
training loss: 11.164113998413086
2022-07-20 18:36:22 itr: 620
training loss: 11.171232223510742
2022-07-20 18:38:31 itr: 630
training loss: 11.179071426391602
2022-07-20 18:40:42 itr: 640
training loss: 11.210580825805664
2022-07-20 18:42:52 itr: 650
training loss: 11.16954231262207
2022-07-20 18:45:03 itr: 660
training loss: 11.152676582336426
2022-07-20 18:47:12 itr: 670
training loss: 11.219134330749512
2022-07-20 18:49:22 itr: 680
training loss: 11.204875946044922
2022-07-20 18:51:31 itr: 690
training loss: 11.201945304870605
2022-07-20 18:53:41 itr: 700
training loss: 11.215643882751465
2022-07-20 18:55:50 itr: 710
training loss: 11.196907997131348
2022-07-20 18:57:59 itr: 720
training loss: 11.237550735473633
2022-07-20 19:00:08 itr: 730
training loss: 11.23690414428711
2022-07-20 19:02:18 itr: 740
training loss: 11.215535163879395
2022-07-20 19:04:27 itr: 750
training loss: 11.24667739868164
2022-07-20 19:06:36 itr: 760
training loss: 11.216133117675781
2022-07-20 19:08:46 itr: 770
training loss: 11.223968505859375
2022-07-20 19:10:55 itr: 780
training loss: 11.29843521118164
2022-07-20 19:13:04 itr: 790
training loss: 11.286629676818848
2022-07-20 19:15:14 itr: 800
training loss: 11.257892608642578
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-800
2022-07-20 19:17:25 itr: 810
training loss: 11.255328178405762
2022-07-20 19:19:35 itr: 820
training loss: 11.322603225708008
2022-07-20 19:21:44 itr: 830
training loss: 11.298935890197754
2022-07-20 19:23:53 itr: 840
training loss: 11.327924728393555
2022-07-20 19:26:03 itr: 850
training loss: 11.312210083007812
2022-07-20 19:28:12 itr: 860
training loss: 11.299781799316406
2022-07-20 19:30:22 itr: 870
training loss: 11.32205581665039
2022-07-20 19:32:31 itr: 880
training loss: 11.329922676086426
2022-07-20 19:34:40 itr: 890
training loss: 11.325695991516113
2022-07-20 19:36:51 itr: 900
training loss: 11.361429214477539
2022-07-20 19:39:01 itr: 910
training loss: 11.403572082519531
2022-07-20 19:41:11 itr: 920
training loss: 11.427583694458008
2022-07-20 19:43:20 itr: 930
training loss: 11.385244369506836
2022-07-20 19:45:30 itr: 940
training loss: 11.41157054901123
2022-07-20 19:47:40 itr: 950
training loss: 11.411453247070312
2022-07-20 19:49:50 itr: 960
training loss: 11.42995548248291
2022-07-20 19:51:59 itr: 970
training loss: 11.44180679321289
2022-07-20 19:54:09 itr: 980
training loss: 11.431081771850586
2022-07-20 19:56:19 itr: 990
training loss: 11.41867446899414
2022-07-20 19:58:29 itr: 1000
training loss: 11.406229019165039
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-1000
2022-07-20 20:00:42 itr: 1010
training loss: 11.400693893432617
2022-07-20 20:02:51 itr: 1020
training loss: 11.452371597290039
2022-07-20 20:05:01 itr: 1030
training loss: 11.457379341125488
2022-07-20 20:07:10 itr: 1040
training loss: 11.463189125061035
2022-07-20 20:09:20 itr: 1050
training loss: 11.451705932617188
2022-07-20 20:11:30 itr: 1060
training loss: 11.5206880569458
2022-07-20 20:13:39 itr: 1070
training loss: 11.484755516052246
2022-07-20 20:15:49 itr: 1080
training loss: 11.474466323852539
2022-07-20 20:17:59 itr: 1090
training loss: 11.539653778076172
2022-07-20 20:20:08 itr: 1100
training loss: 11.569063186645508
2022-07-20 20:22:18 itr: 1110
training loss: 11.540857315063477
2022-07-20 20:24:28 itr: 1120
training loss: 11.550497055053711
2022-07-20 20:26:38 itr: 1130
training loss: 11.560270309448242
2022-07-20 20:28:47 itr: 1140
training loss: 11.574012756347656
2022-07-20 20:30:57 itr: 1150
training loss: 11.590476989746094
2022-07-20 20:33:06 itr: 1160
training loss: 11.575035095214844
2022-07-20 20:35:15 itr: 1170
training loss: 11.59573745727539
2022-07-20 20:37:25 itr: 1180
training loss: 11.563749313354492
2022-07-20 20:39:34 itr: 1190
training loss: 11.535032272338867
2022-07-20 20:41:44 itr: 1200
training loss: 11.579221725463867
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-1200
2022-07-20 20:43:57 itr: 1210
training loss: 11.588436126708984
2022-07-20 20:46:07 itr: 1220
training loss: 11.618549346923828
2022-07-20 20:48:16 itr: 1230
training loss: 11.64849853515625
2022-07-20 20:50:24 itr: 1240
training loss: 11.659379959106445
2022-07-20 20:52:33 itr: 1250
training loss: 11.647619247436523
2022-07-20 20:54:41 itr: 1260
training loss: 11.66647720336914
2022-07-20 20:56:50 itr: 1270
training loss: 11.693375587463379
2022-07-20 20:58:59 itr: 1280
training loss: 11.678010940551758
2022-07-20 21:01:08 itr: 1290
training loss: 11.672370910644531
2022-07-20 21:03:17 itr: 1300
training loss: 11.687519073486328
2022-07-20 21:05:26 itr: 1310
training loss: 11.746942520141602
2022-07-20 21:07:34 itr: 1320
training loss: 11.724048614501953
2022-07-20 21:09:43 itr: 1330
training loss: 11.71841049194336
2022-07-20 21:11:51 itr: 1340
training loss: 11.682901382446289
2022-07-20 21:14:00 itr: 1350
training loss: 11.7247314453125
2022-07-20 21:16:09 itr: 1360
training loss: 11.7384033203125
2022-07-20 21:18:19 itr: 1370
training loss: 11.762125015258789
2022-07-20 21:20:29 itr: 1380
training loss: 11.780306816101074
2022-07-20 21:22:39 itr: 1390
training loss: 11.763229370117188
2022-07-20 21:24:49 itr: 1400
training loss: 11.793573379516602
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-1400
2022-07-20 21:27:03 itr: 1410
training loss: 11.78228759765625
2022-07-20 21:29:13 itr: 1420
training loss: 11.799042701721191
2022-07-20 21:31:22 itr: 1430
training loss: 11.830658912658691
2022-07-20 21:33:31 itr: 1440
training loss: 11.836838722229004
2022-07-20 21:35:40 itr: 1450
training loss: 11.896844863891602
2022-07-20 21:37:50 itr: 1460
training loss: 11.815275192260742
2022-07-20 21:39:59 itr: 1470
training loss: 11.850115776062012
2022-07-20 21:42:09 itr: 1480
training loss: 11.861104965209961
2022-07-20 21:44:18 itr: 1490
training loss: 11.84228515625
2022-07-20 21:46:27 itr: 1500
training loss: 11.879307746887207
2022-07-20 21:48:37 itr: 1510
training loss: 11.885369300842285
2022-07-20 21:50:46 itr: 1520
training loss: 11.927244186401367
2022-07-20 21:52:56 itr: 1530
training loss: 11.909008026123047
2022-07-20 21:55:06 itr: 1540
training loss: 11.954951286315918
2022-07-20 21:57:15 itr: 1550
training loss: 11.912979125976562
2022-07-20 21:59:25 itr: 1560
training loss: 11.981668472290039
2022-07-20 22:01:34 itr: 1570
training loss: 11.967864990234375
2022-07-20 22:03:43 itr: 1580
training loss: 11.994645118713379
2022-07-20 22:05:53 itr: 1590
training loss: 11.997282028198242
2022-07-20 22:08:02 itr: 1600
training loss: 12.04740047454834
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-1600
2022-07-20 22:10:14 itr: 1610
training loss: 12.114812850952148
2022-07-20 22:12:23 itr: 1620
training loss: 12.06993293762207
2022-07-20 22:14:32 itr: 1630
training loss: 12.131711959838867
2022-07-20 22:16:42 itr: 1640
training loss: 12.131242752075195
2022-07-20 22:18:51 itr: 1650
training loss: 12.11427116394043
2022-07-20 22:21:00 itr: 1660
training loss: 12.144923210144043
2022-07-20 22:23:10 itr: 1670
training loss: 12.140013694763184
2022-07-20 22:25:19 itr: 1680
training loss: 12.120026588439941
2022-07-20 22:27:29 itr: 1690
training loss: 12.145398139953613
2022-07-20 22:29:38 itr: 1700
training loss: 12.133756637573242
2022-07-20 22:31:48 itr: 1710
training loss: 12.203365325927734
2022-07-20 22:33:57 itr: 1720
training loss: 12.200090408325195
2022-07-20 22:36:06 itr: 1730
training loss: 12.215360641479492
2022-07-20 22:38:16 itr: 1740
training loss: 12.247570037841797
2022-07-20 22:40:25 itr: 1750
training loss: 12.228277206420898
2022-07-20 22:42:35 itr: 1760
training loss: 12.184917449951172
2022-07-20 22:44:44 itr: 1770
training loss: 12.217273712158203
2022-07-20 22:46:53 itr: 1780
training loss: 12.319969177246094
2022-07-20 22:49:03 itr: 1790
training loss: 12.375995635986328
2022-07-20 22:51:12 itr: 1800
training loss: 12.383621215820312
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-1800
2022-07-20 22:53:24 itr: 1810
training loss: 12.401315689086914
2022-07-20 22:55:34 itr: 1820
training loss: 12.433733940124512
2022-07-20 22:57:44 itr: 1830
training loss: 12.376108169555664
2022-07-20 22:59:53 itr: 1840
training loss: 12.396471977233887
2022-07-20 23:02:03 itr: 1850
training loss: 12.403331756591797
2022-07-20 23:04:12 itr: 1860
training loss: 12.393575668334961
2022-07-20 23:06:21 itr: 1870
training loss: 12.443288803100586
2022-07-20 23:08:30 itr: 1880
training loss: 12.441973686218262
2022-07-20 23:10:39 itr: 1890
training loss: 12.450821876525879
2022-07-20 23:12:48 itr: 1900
training loss: 12.544641494750977
2022-07-20 23:14:58 itr: 1910
training loss: 12.523913383483887
2022-07-20 23:17:08 itr: 1920
training loss: 12.590847969055176
2022-07-20 23:19:17 itr: 1930
training loss: 12.52446174621582
2022-07-20 23:21:27 itr: 1940
training loss: 12.589191436767578
2022-07-20 23:23:36 itr: 1950
training loss: 12.615442276000977
2022-07-20 23:25:46 itr: 1960
training loss: 12.65428638458252
2022-07-20 23:27:56 itr: 1970
training loss: 12.673603057861328
2022-07-20 23:30:05 itr: 1980
training loss: 12.696489334106445
2022-07-20 23:32:14 itr: 1990
training loss: 12.659894943237305
2022-07-20 23:34:24 itr: 2000
training loss: 12.685664176940918
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-2000
2022-07-20 23:36:38 itr: 2010
training loss: 12.686225891113281
2022-07-20 23:38:48 itr: 2020
training loss: 12.715846061706543
2022-07-20 23:40:58 itr: 2030
training loss: 12.797714233398438
2022-07-20 23:43:08 itr: 2040
training loss: 12.844743728637695
2022-07-20 23:45:19 itr: 2050
training loss: 12.796147346496582
2022-07-20 23:47:29 itr: 2060
training loss: 12.795801162719727
2022-07-20 23:49:40 itr: 2070
training loss: 12.843844413757324
2022-07-20 23:51:50 itr: 2080
training loss: 12.857454299926758
2022-07-20 23:54:00 itr: 2090
training loss: 12.8385591506958
2022-07-20 23:56:11 itr: 2100
training loss: 12.896021842956543
2022-07-20 23:58:22 itr: 2110
training loss: 12.93188762664795
2022-07-21 00:00:32 itr: 2120
training loss: 12.897573471069336
2022-07-21 00:02:42 itr: 2130
training loss: 12.884628295898438
2022-07-21 00:04:52 itr: 2140
training loss: 12.94729995727539
2022-07-21 00:07:03 itr: 2150
training loss: 12.92467212677002
2022-07-21 00:09:13 itr: 2160
training loss: 12.99101734161377
2022-07-21 00:11:23 itr: 2170
training loss: 12.970752716064453
2022-07-21 00:13:33 itr: 2180
training loss: 12.992746353149414
2022-07-21 00:15:44 itr: 2190
training loss: 13.001426696777344
2022-07-21 00:17:54 itr: 2200
training loss: 13.004440307617188
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-2200
2022-07-21 00:20:06 itr: 2210
training loss: 13.010180473327637
2022-07-21 00:22:16 itr: 2220
training loss: 13.03270149230957
2022-07-21 00:24:26 itr: 2230
training loss: 13.077774047851562
2022-07-21 00:26:36 itr: 2240
training loss: 13.126116752624512
2022-07-21 00:28:45 itr: 2250
training loss: 13.004638671875
2022-07-21 00:30:55 itr: 2260
training loss: 13.126102447509766
2022-07-21 00:33:05 itr: 2270
training loss: 13.170143127441406
2022-07-21 00:35:15 itr: 2280
training loss: 13.178784370422363
2022-07-21 00:37:24 itr: 2290
training loss: 13.182767868041992
2022-07-21 00:39:34 itr: 2300
training loss: 13.213815689086914
2022-07-21 00:41:44 itr: 2310
training loss: 13.226232528686523
2022-07-21 00:43:54 itr: 2320
training loss: 13.254327774047852
2022-07-21 00:46:04 itr: 2330
training loss: 13.242815017700195
2022-07-21 00:48:13 itr: 2340
training loss: 13.252799987792969
2022-07-21 00:50:23 itr: 2350
training loss: 13.309598922729492
2022-07-21 00:52:33 itr: 2360
training loss: 13.27081298828125
2022-07-21 00:54:43 itr: 2370
training loss: 13.342340469360352
2022-07-21 00:56:53 itr: 2380
training loss: 13.302600860595703
2022-07-21 00:59:02 itr: 2390
training loss: 13.37232494354248
2022-07-21 01:01:11 itr: 2400
training loss: 13.442721366882324
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-2400
2022-07-21 01:03:24 itr: 2410
training loss: 13.41036605834961
2022-07-21 01:05:35 itr: 2420
training loss: 13.548336029052734
2022-07-21 01:07:46 itr: 2430
training loss: 13.655803680419922
2022-07-21 01:09:56 itr: 2440
training loss: 13.613046646118164
2022-07-21 01:12:07 itr: 2450
training loss: 13.552473068237305
2022-07-21 01:14:18 itr: 2460
training loss: 13.549138069152832
2022-07-21 01:16:28 itr: 2470
training loss: 13.627569198608398
2022-07-21 01:18:39 itr: 2480
training loss: 13.675405502319336
2022-07-21 01:20:50 itr: 2490
training loss: 13.716275215148926
2022-07-21 01:23:01 itr: 2500
training loss: 13.732490539550781
2022-07-21 01:25:11 itr: 2510
training loss: 13.756769180297852
2022-07-21 01:27:22 itr: 2520
training loss: 13.771628379821777
2022-07-21 01:29:33 itr: 2530
training loss: 13.840117454528809
2022-07-21 01:31:43 itr: 2540
training loss: 13.922004699707031
2022-07-21 01:33:54 itr: 2550
training loss: 13.975396156311035
2022-07-21 01:36:04 itr: 2560
training loss: 14.024613380432129
2022-07-21 01:38:15 itr: 2570
training loss: 14.067636489868164
2022-07-21 01:40:25 itr: 2580
training loss: 14.082366943359375
2022-07-21 01:42:36 itr: 2590
training loss: 14.136770248413086
2022-07-21 01:44:46 itr: 2600
training loss: 14.189205169677734
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-2600
2022-07-21 01:46:59 itr: 2610
training loss: 14.119548797607422
2022-07-21 01:49:10 itr: 2620
training loss: 14.195266723632812
2022-07-21 01:51:20 itr: 2630
training loss: 14.285947799682617
2022-07-21 01:53:31 itr: 2640
training loss: 14.278826713562012
2022-07-21 01:55:42 itr: 2650
training loss: 14.289302825927734
2022-07-21 01:57:52 itr: 2660
training loss: 14.374494552612305
2022-07-21 02:00:03 itr: 2670
training loss: 14.370421409606934
2022-07-21 02:02:13 itr: 2680
training loss: 14.44416332244873
2022-07-21 02:04:24 itr: 2690
training loss: 14.414421081542969
2022-07-21 02:06:35 itr: 2700
training loss: 14.461685180664062
2022-07-21 02:08:46 itr: 2710
training loss: 14.522489547729492
2022-07-21 02:10:56 itr: 2720
training loss: 14.562963485717773
2022-07-21 02:13:06 itr: 2730
training loss: 14.625526428222656
2022-07-21 02:15:17 itr: 2740
training loss: 14.659747123718262
2022-07-21 02:17:27 itr: 2750
training loss: 14.682402610778809
2022-07-21 02:19:38 itr: 2760
training loss: 14.698158264160156
2022-07-21 02:21:49 itr: 2770
training loss: 14.749838829040527
2022-07-21 02:23:59 itr: 2780
training loss: 14.775943756103516
2022-07-21 02:26:10 itr: 2790
training loss: 14.930340766906738
2022-07-21 02:28:20 itr: 2800
training loss: 14.906503677368164
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-2800
2022-07-21 02:30:34 itr: 2810
training loss: 14.853910446166992
2022-07-21 02:32:44 itr: 2820
training loss: 14.939712524414062
2022-07-21 02:34:55 itr: 2830
training loss: 14.995441436767578
2022-07-21 02:37:05 itr: 2840
training loss: 15.003959655761719
2022-07-21 02:39:16 itr: 2850
training loss: 15.046857833862305
2022-07-21 02:41:26 itr: 2860
training loss: 15.065073013305664
2022-07-21 02:43:37 itr: 2870
training loss: 15.068110466003418
2022-07-21 02:45:47 itr: 2880
training loss: 15.039731979370117
2022-07-21 02:47:58 itr: 2890
training loss: 15.246308326721191
2022-07-21 02:50:09 itr: 2900
training loss: 15.355506896972656
2022-07-21 02:52:19 itr: 2910
training loss: 15.458518028259277
2022-07-21 02:54:30 itr: 2920
training loss: 15.469322204589844
2022-07-21 02:56:41 itr: 2930
training loss: 15.629232406616211
2022-07-21 02:58:51 itr: 2940
training loss: 15.606119155883789
2022-07-21 03:01:02 itr: 2950
training loss: 15.553337097167969
2022-07-21 03:03:13 itr: 2960
training loss: 15.643044471740723
2022-07-21 03:05:23 itr: 2970
training loss: 15.801573753356934
2022-07-21 03:07:34 itr: 2980
training loss: 15.888097763061523
2022-07-21 03:09:45 itr: 2990
training loss: 16.023143768310547
2022-07-21 03:11:55 itr: 3000
training loss: 16.141483306884766
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-3000
2022-07-21 03:14:08 itr: 3010
training loss: 16.13766098022461
2022-07-21 03:16:17 itr: 3020
training loss: 16.273914337158203
2022-07-21 03:18:26 itr: 3030
training loss: 16.260353088378906
2022-07-21 03:20:36 itr: 3040
training loss: 16.35077667236328
2022-07-21 03:22:45 itr: 3050
training loss: 16.362083435058594
2022-07-21 03:24:55 itr: 3060
training loss: 16.449260711669922
2022-07-21 03:27:04 itr: 3070
training loss: 16.55137825012207
2022-07-21 03:29:14 itr: 3080
training loss: 16.374500274658203
2022-07-21 03:31:23 itr: 3090
training loss: 16.628387451171875
2022-07-21 03:33:33 itr: 3100
training loss: 16.592674255371094
2022-07-21 03:35:42 itr: 3110
training loss: 16.804149627685547
2022-07-21 03:37:52 itr: 3120
training loss: 16.766992568969727
2022-07-21 03:40:02 itr: 3130
training loss: 16.82495880126953
2022-07-21 03:42:12 itr: 3140
training loss: 16.888019561767578
2022-07-21 03:44:22 itr: 3150
training loss: 16.988969802856445
2022-07-21 03:46:31 itr: 3160
training loss: 17.059179306030273
2022-07-21 03:48:41 itr: 3170
training loss: 17.11578369140625
2022-07-21 03:50:51 itr: 3180
training loss: 17.183765411376953
2022-07-21 03:53:01 itr: 3190
training loss: 17.282230377197266
2022-07-21 03:55:11 itr: 3200
training loss: 17.268402099609375
save model to /work/08589/hvtran/ls6/ERA5_PredRNN-main/predrnn-pytorch/checkpoints/era5_predrnn/model.ckpt-3200
2022-07-21 03:57:22 itr: 3210
training loss: 17.35247039794922
2022-07-21 03:59:31 itr: 3220
training loss: 17.50818634033203
2022-07-21 04:01:40 itr: 3230
training loss: 17.494205474853516
2022-07-21 04:03:49 itr: 3240
training loss: 17.61989402770996
2022-07-21 04:05:58 itr: 3250
training loss: 17.649402618408203
2022-07-21 04:08:08 itr: 3260
training loss: 17.802078247070312
2022-07-21 04:10:17 itr: 3270
training loss: 17.92675018310547
2022-07-21 04:12:27 itr: 3280
training loss: 17.99465560913086
2022-07-21 04:14:36 itr: 3290
training loss: 18.013601303100586
2022-07-21 04:16:46 itr: 3300
training loss: 18.075271606445312
2022-07-21 04:18:56 itr: 3310
training loss: 18.26529312133789
slurmstepd: error: *** JOB 285420 ON c304-001 CANCELLED AT 2022-07-21T04:19:41 DUE TO TIME LIMIT ***
